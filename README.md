# üß† Reconstrucci√≥n de Im√°genes Mentales desde Actividad Cerebral

Proyecto que convierte se√±ales fMRI del cerebro humano en im√°genes visuales reconstruidas.

## Descripci√≥n

Este proyecto implementa un sistema de decodificaci√≥n neuronal que:
- **Lee se√±ales cerebrales** (fMRI) de personas viendo im√°genes
- **Extrae caracter√≠sticas visuales** mediante redes neuronales (CLIP, VGG19)
- **Reconstruye las im√°genes mentales** usando un generador VQGAN optimizado

El sistema permite "ver" qu√© est√° visualizando una persona analizando solo su actividad cerebral.

## Caracter√≠sticas

‚úÖ **Paper-Accurate Implementation** - Algoritmo validado cient√≠ficamente  
‚úÖ **CLIP Augmentation** - 32 crops aumentados para m√°xima calidad  
‚úÖ **Mean Feature Subtraction** - Elimina bias para mejor convergencia  
‚úÖ **Correlation Loss** - M√°s robusto que MSE en espacios de alta dimensi√≥n  
‚úÖ Procesa datos de 3 sujetos (S01, S02, S03) con 26 im√°genes cada uno  
‚úÖ Pipeline completo end-to-end desde features cerebrales a im√°genes  
‚úÖ Descarga autom√°tica de modelos pre-entrenados (~4GB)  
‚úÖ **Soporte GPU** (CUDA) - 4x m√°s r√°pido que CPU  
‚úÖ Configuraci√≥n flexible con m√∫ltiples modos de calidad

## Diferencias con la Investigaci√≥n Original

Este proyecto **reimplementa y mejora** el algoritmo del paper cient√≠fico:

**Mejoras implementadas**:
- ‚úÖ **Multiplataforma** - Funciona en Windows/Linux/Mac (original solo Linux)
- ‚úÖ **GPU Auto-detection** - Detecta y usa CUDA autom√°ticamente
- ‚úÖ **Configuraci√≥n Flexible** - 3 modos de calidad (fast/standard/high_quality)
- ‚úÖ **Documentaci√≥n Completa** - Gu√≠as paso a paso en espa√±ol
- ‚úÖ **Verificaci√≥n Autom√°tica** - Script `check_setup.py` valida instalaci√≥n
- ‚úÖ **Logging Detallado** - Monitoreo de progreso en tiempo real

**Algoritmo id√©ntico al paper**:
- ‚úÖ CLIP Augmentation (32 crops con transformaciones aleatorias)
- ‚úÖ CLIP Normalization espec√≠fica [0.4814, 0.4578, 0.4082]
- ‚úÖ Mean Feature Subtraction para CLIP y VGG
- ‚úÖ Correlation Loss en lugar de MSE
- ‚úÖ Langevin Dynamics con ruido gaussiano cada 10 iteraciones

## Quick Start

```bash
# 1. Clonar repositorios necesarios
git clone https://github.com/nkmjm/mental_img_recon.git mental_img_recon-main
git clone https://github.com/CompVis/taming-transformers.git taming-transformers-master
git clone https://github.com/openai/CLIP.git CLIP-main

# 2. Instalar dependencias (Python 3.12)
py -3.12 -m pip install -r requirements_py312.txt

# 3. Configurar PROJECT_ROOT en config.py (l√≠nea 22)
# Ajustar la ruta a tu ubicaci√≥n del proyecto

# 4. Verificar instalaci√≥n
py -3.12 check_setup.py

# 5. Ejecutar pipeline
py -3.12 main_local_decoder.py

# 6. Ver resultados
explorer output_reconstructions  # Windows
# open output_reconstructions    # Mac
# xdg-open output_reconstructions # Linux
```

> **‚úÖ VENTAJA**: Los fixes de compatibilidad se aplican **AUTOM√ÅTICAMENTE** al ejecutar:
> - `patch_taming.py` - Arregla `torch._six` en taming-transformers SIN modificar el repo
> - `pytorch_lightning_compat.py` - Arregla PyTorch Lightning 2.x
> 
> **No necesitas editar manualmente ning√∫n archivo de repositorios externos**

Para instrucciones detalladas de instalaci√≥n y configuraci√≥n, consulta **[SETUP.md](SETUP.md)**.

## Estructura del Proyecto

```
ACECOM-Project/
‚îú‚îÄ‚îÄ features/                    # Dataset (extra√≠do de features.tar.gz)
‚îÇ   ‚îú‚îÄ‚îÄ decoded_features/        # Features cerebrales por sujeto
‚îÇ   ‚îî‚îÄ‚îÄ meanDNNfeature/          # Features promedio
‚îú‚îÄ‚îÄ mental_img_recon-main/       # Repositorio base
‚îú‚îÄ‚îÄ taming-transformers-master/  # Arquitectura VQGAN
‚îú‚îÄ‚îÄ CLIP-main/                   # Arquitectura CLIP
‚îú‚îÄ‚îÄ main_local_decoder.py        # Script principal
‚îú‚îÄ‚îÄ config.py                    # Configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ output_reconstructions/      # Im√°genes generadas (se crea autom√°ticamente)
```

## Requisitos del Sistema

- **Python**: 3.12 (estable, recomendado) o 3.8+
- **RAM**: 8GB m√≠nimo, 16GB recomendado
- **GPU**: NVIDIA con CUDA (opcional, acelera 4x)
- **Espacio**: ~15GB (dataset + modelos)

## Scripts Principales

### `main_local_decoder.py`
Pipeline de reconstrucci√≥n completo. Lee features cerebrales y genera im√°genes.

**Uso b√°sico:**
```bash
py -3.12 main_local_decoder.py
```

**Configuraci√≥n r√°pida/est√°ndar/alta:**
Edita `ACTIVE_CONFIG` en `config.py` (`'fast'` / `'standard'` / `'high_quality'`)

### `utils_visualization.py`
Genera cuadr√≠culas comparativas y reportes HTML.

```bash
py -3.12 utils_visualization.py
```

### `check_setup.py`
Verifica que todo est√© configurado correctamente antes de ejecutar.

```bash
py -3.12 check_setup.py
```

## Autor

**Proyecto**: ACECOM - Decodificaci√≥n de Im√°genes Mentales  
**Estudiante**: Alvaro Jesus Taipe Cotrina  
**Instituci√≥n**: Universidad Nacional de Ingenier√≠a (UNI)  
**A√±o**: 2025

## Bibliograf√≠a e Inspiraci√≥n

### Paper Cient√≠fico Original

Este proyecto est√° **inspirado e implementa** el algoritmo descrito en:

> **Koide-Majima, N., Nishimoto, S.** (2024). "Mental image reconstruction from human brain activity: Neural decoding of mental imagery via deep neural network-based Bayesian estimation"

**Fundamento Cient√≠fico**:

El paper propone un m√©todo bayesiano para reconstruir im√°genes mentales:

1. **Codificador Cerebral** (f_enc): Mapea actividad fMRI ‚Üí espacio de embeddings CLIP/VGG
2. **Optimizaci√≥n Bayesiana**: Minimiza `L = L_CLIP + Œª¬∑L_VGG` usando din√°mica de Langevin
3. **Generador VQGAN**: Sintetiza imagen final desde el espacio latente optimizado

**Ecuaci√≥n Central**:
```
Pr(I|Œ¶_VGG, Œ¶_CLIP) ‚àù Pr(Œ¶_VGG|I) √ó Pr(Œ¶_CLIP|I) √ó Pr(I)
                       ‚Üë              ‚Üë              ‚Üë
                  Likelihood      Likelihood      Prior
                  (Visual)       (Sem√°ntico)    (Natural)
```

### Repositorios Base

Este proyecto utiliza y se basa en los siguientes repositorios:

- **Implementaci√≥n Original**: [nkmjm/mental_img_recon](https://github.com/nkmjm/mental_img_recon)
  - C√≥digo fuente del paper
  - Dataset de features cerebrales pre-extra√≠das
  
- **VQGAN**: [CompVis/taming-transformers](https://github.com/CompVis/taming-transformers)
  - Esser et al. (2021) "Taming Transformers for High-Resolution Image Synthesis"
  - Generador de im√°genes de alta calidad
  
- **CLIP**: [openai/CLIP](https://github.com/openai/CLIP)
  - Radford et al. (2021) "Learning Transferable Visual Models From Natural Language Supervision"
  - Espacio latente multimodal

### Dataset  
**datos/fmri**: (https://drive.google.com/uc?id=1Q7TVsVbASMqnDYfFjFzo2SV6njExu8qq)
El dataset `features.tar.gz` (NO incluido en GitHub por tama√±o) contiene:

- Features cerebrales pre-extra√≠das de fMRI
- 3 sujetos (S01, S02, S03)
- 26 im√°genes por sujeto
- Features CLIP (512-d) y VGG19 (4096-d por capa)

**Obtenci√≥n**: Descarga desde el repositorio original [nkmjm/mental_img_recon](https://github.com/nkmjm/mental_img_recon)

## Licencia

Este proyecto es con fines acad√©micos y de investigaci√≥n. Los modelos pre-entrenados (VQGAN, CLIP, VGG19) mantienen sus licencias originales.

---

**üöÄ Para comenzar, consulta [SETUP.md](SETUP.md) para instrucciones detalladas de instalaci√≥n.**

